#!/usr/bin/env python3

""" 
This script labels each log event with the ATT&CK Tatic and Technique that generated it.
Alternatively, if a pcap file (*.pcap) is given instead of an attack log (*.csv) then
    this script will simply trim the log down to the time frame of the pcap.

    Assumptions: 
        1) All log events within an attack timeline are the result of malicious activity.
        2) All log events have the field 'epoch_time', which contains the timestamp in epoch time format
        
    Requirements:
        1) "splunk_log" - A csv file exported from splunk, or a directory of splunk logs with *.csv extension.
        2) "atk_log" - ATT&CK execution log file, as generated by the `get_attack_data` search in splunk. 
                This log only exists in Splunk Attack Range.  
            - 
    
    USAGE: 
    python label_event_log.py [-p] splunk_log.cvs atk_log.csv 
    OR
    python label_event_log.py T1563.002.csv T1563.002.pcap
"""

import os
import argparse
from xmlrpc.client import Boolean
import pandas as pd
from scapy.all import *


def label_log(log, atk):
    
    # Trim log events that are outside the time of interest:
    #   1) Events more than 10s before the first execution time
    #   2) Events after the last execution time
    log.drop(log[log._time < (atk._time.iloc[-1] - pd.Timedelta(seconds=10))].index, inplace=True)
    log.drop(log[log._time > atk._time.iloc[1]].index, inplace=True)
        
    # Assign label to matching time events in atc_log
    log = log.merge(atk, on='_time', how='left')

    # Since atk_log execution timestamps are written after completion of the technique, we can assume 
    # log events that happened immediately before the execution timestamp is a result of the technique
    # being executed.
    log['Tactic'].fillna(method='ffill', inplace=True)
    log['Technique'].fillna(method='ffill', inplace=True)

    # print("Final log dataframe...")
    # print(log['_time'])
    return log

def import_csv(log_path: str, preview: Boolean=False, **kwargs):
    # Load log_file and delete empty columns
    if not preview: 
        log_df = pd.read_csv(log_path, **kwargs) 
    else: 
        log_df = pd.read_csv(log_path, nrows=250, **kwargs)
    log_df.replace("", float("NaN"), inplace=True)
    log_df.dropna(how='all', axis=1, inplace=True)

    # Convert time string to datetime
    log_df['_time'] = pd.to_datetime(log_df['_time'])
    
    # Convert epoch_time from string to float
    log_df['epoch_time'] = pd.to_numeric(log_df['epoch_time'])

    # Sort by timestamps
    log_df = log_df.sort_values('_time', ascending=True)
    
    print("Imported log")
    
    return log_df

def trim_to_pcap(log , pcap):
    print("First PCAP Timestamp: ", pcap[0].time)
    print("Last PACAP Timestamp:", pcap[-1].time)

    return log[ ((log['epoch_time'] > pcap[0].time) & (log['epoch_time'] < pcap[-1].time)) ]

def main():
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument(
        "--preview", '-p', 
        default=False, action='store_true',
        help="Just import the first 250 lines of log. Note:this might return 0 useable events if the first \
            250 lines are not within the relevant timeline."
    )
    parser.add_argument(
        "splunk_log", 
        help='A csv file exported from splunk, or a directory of splunk logs with *.csv extension.'
    )
    parser.add_argument(
        "reference_file", 
        help='reference_file can be either of: 1) ATT&CK execution log file, as generated by the `get_attack_data` search in splunk \
                OR 2) A PCAP file'        
    )
    
    ns = parser.parse_args()
    
    atk_df = None
    pcap = None

    if ns.reference_file.endswith(".csv") :
        # Load atk_log, taking only columns of interested
        atk_df = import_csv(ns.reference_file, usecols=['_time','Tactic', 'Technique'])
    else: # Assume PCAP file provided
        pcap = rdpcap(ns.reference_file)



    # Directory given - label all unlabeled *.csv files
    if os.path.isdir(ns.splunk_log):  
        print("\nLabeling all *.csv files in directory.")  
        files = [f for f in os.scandir(ns.splunk_log) if f.is_file()]
        logs = [f for f in files if f.name.endswith(".csv") and not f.name.startswith("LABELED_")]
        for log in logs:            
            log_df = import_csv(log, ns.preview)

            if atk_df is not None:
                # Label logs
                log_df = label_log(log_df, atk_df)
            else: 
                # Trim logs
                log_df = trim_to_pcap(log_df, pcap)
            
            # Save log to file
            [log_dir, log_name] = os.path.split(os.path.abspath(log))
            new_filename = os.path.join(log_dir, "LABELED_" + log_name)
            log_df.to_csv(new_filename)
            print("Wrote to file: " + new_filename)
    # File given
    elif os.path.isfile(ns.splunk_log):  
        log_df = import_csv(ns.splunk_log, ns.preview)
        
        if atk_df is not None:
            # Label logs
            log_df = label_log(log_df, atk_df)
        else: 
            # Trim logs
            log_df = trim_to_pcap(log_df, pcap)
            
        # Save log to file
        [log_dir, log_name] = os.path.split(os.path.abspath(ns.splunk_log))
        new_filename = os.path.join(log_dir, "LABELED_" + log_name)
        log_df.to_csv(new_filename)
        # print(log_df[['_time','Technique']])
        print("Wrote to file: " + new_filename)

    else:  
        print("splunk_log must be a file or directory!" )
        return

if __name__ == "__main__":
    main()
